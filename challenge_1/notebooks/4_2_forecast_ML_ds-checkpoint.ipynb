{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Multi-step forecasts\n",
    "## Forecasting 7 days ahead using half hour dataset\n",
    "### Coding: utf-8\n",
    "### This code could be optimized to use multiprocessing with ProcessPoolExecutor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.linear_model import Lars\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'forecast_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-0d955a934113>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../scripts/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mforecast_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mav_scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'forecast_utils'"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"../scripts/\")\n",
    "from forecast_utils import av_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='../input/merged_data/'\n",
    "MAC_NAME='mac000230'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAILY_SAMPLE_RATE=48\n",
    "#one week ahead\n",
    "FORECAST_DAYS=7\n",
    "TEST_WEEKS = 1\n",
    "TEST_SAMPLES=TEST_WEEKS*FORECAST_DAYS*DAILY_SAMPLE_RATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#split a univariate dataset into train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data):\n",
    "    #use last n weeks for test (enough for validation and test)\n",
    "    week_hh = FORECAST_DAYS*DAILY_SAMPLE_RATE\n",
    "    train=data[:-TEST_SAMPLES]\n",
    "    test = data[-TEST_SAMPLES:]\n",
    "    # restructure into windows of weekly data\n",
    "    train = array(split(train, len(train)/week_hh))\n",
    "    test = array(split(test, len(test)/week_hh))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "    print('>evaluate_forecasts()')\n",
    "    scores = list()\n",
    "    # calculate an RMSE score for each day\n",
    "    for i in range(actual.shape[1]):\n",
    "        # calculate mse\n",
    "        mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "        # calculate rmse\n",
    "        rmse = sqrt(mse)\n",
    "        # store\n",
    "        scores.append(rmse)\n",
    "    # calculate overall RMSE\n",
    "    s = 0\n",
    "    for row in range(actual.shape[0]):\n",
    "        for col in range(actual.shape[1]):\n",
    "            s += (actual[row, col] - predicted[row, col])**2\n",
    "    score = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "    return score, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize scores\n",
    "def print_scores(name, score, scores):\n",
    "    #s_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "    #print('%s: [%.3f] %s' % ('Half hourly', score, s_scores))\n",
    "    n_chunks = len(scores)/DAILY_SAMPLE_RATE\n",
    "    print(type(scores))\n",
    "    scores_chunked = np.array_split(scores, n_chunks)\n",
    "    av_scores = []\n",
    "    for chunk in scores_chunked:\n",
    "        av_scores.append(np.average(chunk))\n",
    "    w_scores = ', '.join(['%.1f' % s for s in av_scores])\n",
    "    print('%s: [%.3f] %s' % (name, score, w_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_best_algo(name_score):\n",
    "    best_alg = ''\n",
    "    best_score = 0\n",
    "    for i, n_s in enumerate(name_score):\n",
    "        if i == 0:\n",
    "            best_alg=n_s[0]\n",
    "            best_score=n_s[1]\n",
    "        else:\n",
    "            if n_s[1]<best_score:\n",
    "                best_alg=n_s[0]\n",
    "                best_score=n_s[1]\n",
    "    print('Best overall algorithm: {0}'.format(best_alg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a list of ml models\n",
    "def get_models(models=dict()):\n",
    "    # linear models\n",
    "    models['linear regression'] = LinearRegression()\n",
    "    models['lasso'] = Lasso()\n",
    "    models['ridge'] = Ridge()\n",
    "    models['elastic net'] = ElasticNet()\n",
    "    models['huber regressor'] = HuberRegressor()\n",
    "    #models['lars'] = Lars()\n",
    "    models['lasso lars'] = LassoLars()\n",
    "    models['passive aggressive regressor'] = PassiveAggressiveRegressor(max_iter=1000, tol=1e-3)\n",
    "    models['ranscac regressor'] = RANSACRegressor(min_samples=7)\n",
    "    models['sgd regressor'] = SGDRegressor(max_iter=5000, tol=1e-3)\n",
    "    print('Defined %d models' % len(models))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a feature preparation pipeline for a model\n",
    "def make_pipeline(model):\n",
    "    steps = list()\n",
    "    # standardization\n",
    "    steps.append(('standardize', StandardScaler()))\n",
    "    # normalization\n",
    "    steps.append(('normalize', MinMaxScaler()))\n",
    "    # the model\n",
    "    steps.append(('model', model))\n",
    "    # create pipeline\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert history into inputs and outputs\n",
    "def to_supervised(history, output_ix, data_column):\n",
    "    X, y = list(), list()\n",
    "    # step over the entire history one time step at a time\n",
    "    for i in range(len(history)-1):\n",
    "        for j in range(DAILY_SAMPLE_RATE):\n",
    "            #as we are sub-sampling individual days by the DAILY_SAMPLE_RATE\n",
    "            #we need to use part of the next week to predict the current step\n",
    "            rolling_window = history[i][j:,data_column]\n",
    "            rolling_window_next = history[i+ 1][:j, data_column]\n",
    "            rolling_window = np.append(rolling_window, rolling_window_next)\n",
    "            X.append(rolling_window)\n",
    "            y.append(history[i + 1][output_ix,data_column])\n",
    "    assert(len(X)==len(y))\n",
    "    assert(len(X[0])==DAILY_SAMPLE_RATE*FORECAST_DAYS)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model and make a forecast\n",
    "def sklearn_predict(model, history, data_column):\n",
    "    yhat_sequence = list()\n",
    "    # fit a model for each step\n",
    "    for i in range(FORECAST_DAYS*DAILY_SAMPLE_RATE):\n",
    "        # prepare data\n",
    "        train_x, train_y = to_supervised(history, i, data_column)\n",
    "        # make pipeline\n",
    "        pipeline = make_pipeline(model)\n",
    "        # fit the model\n",
    "        pipeline.fit(train_x, train_y)\n",
    "        # forecast\n",
    "        x_input = array(train_x[-1, :]).reshape(1,FORECAST_DAYS*DAILY_SAMPLE_RATE)\n",
    "        yhat = pipeline.predict(x_input)[0]\n",
    "        # store\n",
    "        yhat_sequence.append(yhat)\n",
    "    return yhat_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(model, train, test, data_column=0):\n",
    "    '''By default use first column as the data column we train and predict for'''\n",
    "    # history is a list of weekly data\n",
    "    history = [x for x in train]\n",
    "    # walk-forward validation over each week\n",
    "    predictions = list()\n",
    "    for i in range(len(test)):\n",
    "        # predict the week\n",
    "        yhat_sequence = sklearn_predict(model, history, data_column)\n",
    "        # store the predictions\n",
    "        predictions.append(yhat_sequence)\n",
    "        # get real observation and add to history for predicting the next week\n",
    "        history.append(test[i, :])\n",
    "    predictions = array(predictions)\n",
    "    # evaluate predictions days for each week\n",
    "    actuals = test[:, :, data_column]\n",
    "    score, scores = evaluate_forecasts(actuals, predictions)\n",
    "    return score, scores, actuals, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(name, model):\n",
    "    print('>eval_model() {0}'.format(name))\n",
    "    # evaluate and get scores, energy(kWh/hh) is the first column in our dataset\n",
    "    score, scores, actuals, predictions = evaluate_model(model, train, test, data_column=0)\n",
    "    # summarize scores\n",
    "    print_scores(name, score, scores)\n",
    "    av = av_scores(scores, DAILY_SAMPLE_RATE)\n",
    "    # plot scores\n",
    "    return av, actuals, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Read in data\n",
    "dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "dataset = pd.read_csv('{0}LCLid/clean_{1}.csv'.format(PATH, MAC_NAME), parse_dates=['day_time'], date_parser=dateparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsample for testing\n",
    "#dataset = dataset[-1*(4*DAILY_SAMPLE_RATE*FORECAST_DAYS):]\n",
    "\n",
    "dataset.set_index(['day_time'],inplace=True)\n",
    "#original data is to 3 decimal places\n",
    "dataset['energy(kWh/hh)'] = dataset['energy(kWh/hh)'].round(3)\n",
    "dataset['energy(Wh/hh)'] = dataset['energy(kWh/hh)'].multiply(1000)\n",
    "\n",
    "dataset =  dataset[['energy(Wh/hh)', 'temperature', 'humidity']]\n",
    "dataset=dataset.fillna(0)\n",
    "\n",
    "ts = dataset['energy(Wh/hh)'].values\n",
    "ds = dataset.index.values\n",
    "\n",
    "plt.plot(ds, ts)\n",
    "plt.show(block=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "train, test = split_dataset(dataset.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prepare the models to evaluate\n",
    "models = get_models()\n",
    "\n",
    "days = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\n",
    "score_list = []\n",
    "\n",
    "\n",
    "avs=[]\n",
    "actuals=[]\n",
    "predictions=[]\n",
    "names=[]\n",
    "for name, model in models.items():\n",
    "    av, actual, prediction = eval_model(name, model)\n",
    "    names.append(name)\n",
    "    avs.append(av)\n",
    "    score_list.append((name, av))\n",
    "    predictions.append(prediction)\n",
    "    actuals.append(actual)\n",
    "\n",
    "for name, av in zip(names, avs):\n",
    "    plt.scatter(days, av, marker='o', label=name)\n",
    "    plt.ylabel('RSWE Wh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show plot\n",
    "plt.legend()\n",
    "plt.savefig('plots/4_2_a_{0}_{1}_day_forecast_rmse.png'.format(MAC_NAME, FORECAST_DAYS))\n",
    "plt.show(block=False)\n",
    "\n",
    "i=0\n",
    "hh=range(FORECAST_DAYS*DAILY_SAMPLE_RATE)\n",
    "for name, actual, pred in zip(names, actuals, predictions):\n",
    "    plt.subplot(len(models.items()), 1, i+1)\n",
    "    actual=actual.flatten()\n",
    "    plt.plot(hh, actual, label='actual', color='k')\n",
    "    prediction = pred.flatten()\n",
    "    plt.plot(hh, prediction,  label='predicted', color='r')\n",
    "    plt.title(name, fontsize=10)\n",
    "    plt.ylabel('Wh/hh')\n",
    "    i+=1\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig('plots/4_2_a_{0}_{1}_day_forecast_actuals.png'.format(MAC_NAME, FORECAST_DAYS))\n",
    "plt.show(block=False)\n",
    "\n",
    "\n",
    "\n",
    "best_alg = print_best_algo(score_list)\n",
    "\n",
    "# Display history and forecasts on same plot\n",
    "\n",
    "# Run models on daily dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
