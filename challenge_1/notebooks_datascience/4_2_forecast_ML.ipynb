{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machne Learning Multi-step forecasts\n",
    "\n",
    "Forecasting 7 days ahead using half hourly dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.linear_model import Lars\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sys.path.append(\"../scripts/\")\n",
    "from forecast_utils import av_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='../input/merged_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAILY_SAMPLE_RATE=48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "    # split into weeks\n",
    "    #use last 8 weeks for test (enough for validation and test)\n",
    "    week_hh = 7*DAILY_SAMPLE_RATE\n",
    "    test_days = 8*week_hh\n",
    "    train, test = data[:-(test_days)], data[test_days:]\n",
    "    print('train: {0}, split weeks: {1}'.format(len(train), len(train)/week_hh))\n",
    "    print('test: {0}, split weeks: {1}'.format(len(test), len(test)/week_hh))\n",
    "    # restructure into windows of weekly data\n",
    "    train = array(split(train, len(train)/week_hh))\n",
    "    test = array(split(test, len(test)/week_hh))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "    scores = list()\n",
    "    # calculate an RMSE score for each day\n",
    "    for i in range(actual.shape[1]):\n",
    "        #print('pred: {0}'.format(predicted[0].shape))\n",
    "        #print('act: {0}'.format(actual[0].shape))\n",
    "        # calculate mse\n",
    "        mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "        # calculate rmse\n",
    "        rmse = sqrt(mse)\n",
    "        # store\n",
    "        scores.append(rmse)\n",
    "    # calculate overall RMSE\n",
    "    s = 0\n",
    "    for row in range(actual.shape[0]):\n",
    "        for col in range(actual.shape[1]):\n",
    "            s += (actual[row, col] - predicted[row, col])**2\n",
    "    score = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "    return score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize scores\n",
    "def print_scores(name, score, scores):\n",
    "    #s_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "    #print('%s: [%.3f] %s' % ('Half hourly', score, s_scores))\n",
    "    n_chunks = len(scores)/DAILY_SAMPLE_RATE\n",
    "    print(type(scores))\n",
    "    scores_chunked = np.array_split(scores, n_chunks)\n",
    "    av_scores = []\n",
    "    for chunk in scores_chunked:\n",
    "        av_scores.append(np.average(chunk))\n",
    "    w_scores = ', '.join(['%.1f' % s for s in av_scores])\n",
    "    print('%s: [%.3f] %s' % (name, score, w_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize scores\n",
    "def print_best_algo(name_score):\n",
    "    best_alg = ''\n",
    "    best_score = 0\n",
    "    for i, n_s in enumerate(name_score):\n",
    "        if i == 0:\n",
    "            best_alg=n_s[0]\n",
    "            best_score=n_s[1]\n",
    "        else:\n",
    "            if n_s[1]<best_score:\n",
    "                best_alg=n_s[0]\n",
    "                best_score=n_s[1]\n",
    "    print('Best overall algorithm: {0}'.format(best_alg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a list of ml models\n",
    "def get_models(models=dict()):\n",
    "    # linear models\n",
    "    models['lr'] = LinearRegression()\n",
    "    models['lasso'] = Lasso()\n",
    "    models['ridge'] = Ridge()\n",
    "    models['en'] = ElasticNet()\n",
    "    models['huber'] = HuberRegressor()\n",
    "    models['lars'] = Lars()\n",
    "    models['llars'] = LassoLars()\n",
    "    models['pa'] = PassiveAggressiveRegressor(max_iter=1000, tol=1e-3)\n",
    "    models['ranscac'] = RANSACRegressor()\n",
    "    models['sgd'] = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "    print('Defined %d models' % len(models))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a feature preparation pipeline for a model\n",
    "def make_pipeline(model):\n",
    "    steps = list()\n",
    "    # standardization\n",
    "    steps.append(('standardize', StandardScaler()))\n",
    "    # normalization\n",
    "    steps.append(('normalize', MinMaxScaler()))\n",
    "    # the model\n",
    "    steps.append(('model', model))\n",
    "    # create pipeline\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert history into inputs and outputs\n",
    "def to_supervised(history, output_ix):\n",
    "    X, y = list(), list()\n",
    "    # step over the entire history one time step at a time\n",
    "    for i in range(len(history)-1):\n",
    "        X.append(history[i][:,0])\n",
    "        y.append(history[i + 1][output_ix,0])\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model and make a forecast\n",
    "def sklearn_predict(model, history):\n",
    "    #print('len: {0}, history[0].shape: {1}'.format(len(history), history[0].shape))\n",
    "    yhat_sequence = list()\n",
    "    # fit a model for each forecast hh\n",
    "    for i in range(7*DAILY_SAMPLE_RATE):\n",
    "        # prepare data\n",
    "        train_x, train_y = to_supervised(history, i)\n",
    "        # make pipeline\n",
    "        pipeline = make_pipeline(model)\n",
    "        # fit the model\n",
    "        pipeline.fit(train_x, train_y)\n",
    "        # forecast\n",
    "        x_input = array(train_x[-1, :]).reshape(1,7*DAILY_SAMPLE_RATE)\n",
    "        yhat = pipeline.predict(x_input)[0]\n",
    "        # store\n",
    "        yhat_sequence.append(yhat)\n",
    "    return yhat_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(model, train, test):\n",
    "    # history is a list of weekly data\n",
    "    history = [x for x in train]\n",
    "    # walk-forward validation over each week\n",
    "    predictions = list()\n",
    "    for i in range(len(test)):\n",
    "        # predict the week\n",
    "        yhat_sequence = sklearn_predict(model, history)\n",
    "        # store the predictions\n",
    "        predictions.append(yhat_sequence)\n",
    "        # get real observation and add to history for predicting the next week\n",
    "        history.append(test[i, :])\n",
    "    predictions = array(predictions)\n",
    "    # evaluate predictions days for each week\n",
    "    score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "    return score, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('{0}LCLid/clean_mac000230.csv'.format(PATH), parse_dates=['day_time'], date_parser=dateparse)\n",
    "dataset.set_index(['day_time'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =  dataset[['energy(kWh/hh)', 'temperature', 'humidity']]\n",
    "dataset=dataset.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 36288, split weeks: 108.0\n",
      "test: 36288, split weeks: 108.0\n",
      "Defined 10 models\n"
     ]
    }
   ],
   "source": [
    "# split into train and test\n",
    "train, test = split_dataset(dataset.values)\n",
    "# prepare the models to evaluate\n",
    "models = get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each model\n",
    "days = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\n",
    "score_list = []\n",
    "for name, model in models.items():\n",
    "    # evaluate and get scores\n",
    "    score, scores = evaluate_model(model, train, test)\n",
    "    score_list.append((name, score))\n",
    "    # summarize scores\n",
    "    print_scores(name, score, scores)\n",
    "    av = av_scores(scores, DAILY_SAMPLE_RATE)\n",
    "    # plot scores\n",
    "    plt.scatter(days, scores, marker='o', label=name)\n",
    "# show plot\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alg = print_best_algo(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "Display history and forecasts on same plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily data\n",
    "\n",
    "Run models on daily dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
